{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_01_saliency_and_grad_cam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDqNK9paSN_Y"
      },
      "source": [
        "# Workshop 01: Saliency and Grad-CAM Examples\n",
        "\n",
        "[![Open In Colab <](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ShawnHymel/ei-workshop-image-data-augmentation/blob/master/workshop_01_saliency_and_grad_cam.ipynb)\n",
        "\n",
        "Navigate to your project on [Edge Impulse](https://www.edgeimpulse.com/). Go to **Dashboard**. Download **TensorFlow SavedModel**. Upload that .zip file to */content/*.\n",
        "\n",
        "Upload an image from your training or test dataset (e.g. 42.png). Change the settings as necessary so that the notebook can find your model file and image sample.\n",
        "\n",
        "Run the notebook to see a saliency map and Grad-CAM heatmap.\n",
        "\n",
        "Saliency maps highlight which pixels in the input image were most important (i.e. most salient) in the decision making process.\n",
        "\n",
        "Grad-CAM looks at the output feature map of the final convolution layer to figure out which areas of the image were the most important in the decision making process.\n",
        "\n",
        "Author: EdgeImpulse, Inc.<br>\n",
        "Modified by: Shawn Hymel<br>\n",
        "Date: September 22, 2021<br>\n",
        "License: [Apache-2.0](apache.org/licenses/LICENSE-2.0)<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGv808ddkd3P"
      },
      "source": [
        "import PIL\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import activations, layers, models, backend\n",
        "from skimage.transform import resize\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MdzYh9ZkmMB"
      },
      "source": [
        "### Settings\n",
        "\n",
        "# Change this based on your .zip model filename\n",
        "MODEL_ZIP_PATH = \"/content/ei-electronic-components-cnn-nn-classifier-tensorflow-savedmodel-model.zip\"\n",
        "\n",
        "# Labels\n",
        "LABELS = [\"background\", \"capacitor\", \"diode\", \"led\", \"resistor\"]\n",
        "\n",
        "# Change this based on your image sample\n",
        "IMAGE_PATH = \"/content/19.png\"\n",
        "TRUE_LABEL = \"resistor\"\n",
        "\n",
        "# Image resolution (Edge Impulse project > Impulse design > Image data)\n",
        "WIDTH = 28\n",
        "HEIGHT = 28\n",
        "\n",
        "# Saved model unzip path (you probably don't need to change this)\n",
        "MODEL_DIR = \"/content/saved_model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l08CYyHvXSa"
      },
      "source": [
        "### Find index of true label in label list\n",
        "true_idx = LABELS.index(TRUE_LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIo3gmXQsuCw"
      },
      "source": [
        "### Unzip model\n",
        "!unzip -q -o \"{MODEL_ZIP_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDqyA0XRknj6"
      },
      "source": [
        "### Load model file\n",
        "model = tf.keras.models.load_model(MODEL_DIR)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJhWpUsBtYf3"
      },
      "source": [
        "### Load image and extract features\n",
        "\n",
        "# Load image\n",
        "img = PIL.Image.open(IMAGE_PATH)\n",
        "\n",
        "# Convert the image to grayscale\n",
        "img = img.convert('L')\n",
        "\n",
        "# Convert the image to a Numpy array\n",
        "img = np.asarray(img)\n",
        "\n",
        "# Resize the image and normalize the values (to be between 0.0 and 1.0)\n",
        "img = resize(img, (HEIGHT, WIDTH), anti_aliasing=True)\n",
        "\n",
        "# Show the ground-truth label\n",
        "print(\"Actual label:\", TRUE_LABEL)\n",
        "\n",
        "# Display image (make sure we're looking at the right thing)\n",
        "plt.imshow(img, cmap='gray', vmin=0.0, vmax=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJaVVmkNqgAj"
      },
      "source": [
        "### The Keras model expects images in a 4D array with dimensions (sample, height, width, channel)\n",
        "\n",
        "# Add extra dimension to the image (placeholder for color channels)\n",
        "img_0 = img.reshape(img.shape + (1,))\n",
        "\n",
        "# Keras expects more than one image (in Numpy array), so convert image(s) to such array\n",
        "images = np.array([img_0])\n",
        "\n",
        "# Print dimensions of inference input\n",
        "print(images.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V430dQ8Wp_b0"
      },
      "source": [
        "### Do a forward pass (inference) with the test image and print the predicted probabilities\n",
        "\n",
        "# Inference\n",
        "preds = model.predict(images)\n",
        "\n",
        "# Print out predictions\n",
        "for i, pred in enumerate(preds[0]):\n",
        "  print(LABELS[i] + \": \" + str(pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgTp5vRMqOXD"
      },
      "source": [
        "### For either algorithm, we need to remove the Softmax activation function of the last layer\n",
        "model.layers[-1].activation = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PIMyTWzHGfA"
      },
      "source": [
        "## Saliency Map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jhNP53mHJBv"
      },
      "source": [
        "### Based on: https://github.com/keisen/tf-keras-vis/blob/master/tf_keras_vis/saliency.py\n",
        "def get_saliency_map(img_array, model, class_idx):\n",
        "\n",
        "  # Gradient calculation requires input to be a tensor\n",
        "  img_tensor = tf.convert_to_tensor(img_array)\n",
        "\n",
        "  # Do a forward pass of model with image and track the computations on the \"tape\"\n",
        "  with tf.GradientTape(watch_accessed_variables=False, persistent=True) as tape:\n",
        "\n",
        "    # Compute (non-softmax) outputs of model with given image\n",
        "    tape.watch(img_tensor)\n",
        "    outputs = model(img_tensor, training=False)\n",
        "\n",
        "    # Get score (predicted value) of actual class\n",
        "    score = outputs[:, true_idx]\n",
        "\n",
        "  # Compute gradients of the loss with respect to the input image\n",
        "  grads = tape.gradient(score, img_tensor)  \n",
        "\n",
        "  # Finds max value in each color channel of the gradient (should be grayscale for this demo)\n",
        "  grads_disp = [np.max(g, axis=-1) for g in grads]\n",
        "\n",
        "  # There should be only one gradient heatmap for this demo\n",
        "  grad_disp = grads_disp[0]\n",
        "\n",
        "  # The absolute value of the gradient shows the effect of change at each pixel\n",
        "  # Source: https://christophm.github.io/interpretable-ml-book/pixel-attribution.html\n",
        "  grad_disp = tf.abs(grad_disp)\n",
        "\n",
        "  # Normalize to between 0 and 1 (use epsilon, a very small float, to prevent divide-by-zero error)\n",
        "  heatmap_min = np.min(grad_disp)\n",
        "  heatmap_max = np.max(grad_disp)\n",
        "  heatmap = (grad_disp - heatmap_min) / (heatmap_max - heatmap_min + tf.keras.backend.epsilon())\n",
        "\n",
        "  return heatmap.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv3ZPnM5HRwg"
      },
      "source": [
        "### Generate saliency map for the given input image\n",
        "saliency_map = get_saliency_map(images, model, true_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55N3wuvNNn5z"
      },
      "source": [
        "### Draw map\n",
        "plt.imshow(saliency_map, cmap='jet', vmin=0.0, vmax=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYGRdsbZQfKH"
      },
      "source": [
        "### Overlay the saliency map on top of the original input image\n",
        "idx = 0\n",
        "ax = plt.subplot()\n",
        "ax.imshow(images[idx,:,:,0], cmap='gray', vmin=0.0, vmax=1.0)\n",
        "ax.imshow(saliency_map, cmap='jet', alpha=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__J5ufo7m_-q"
      },
      "source": [
        "## Grad-CAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o462piKhlDES"
      },
      "source": [
        "### This function comes from https://keras.io/examples/vision/grad_cam/\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "\n",
        "  # First, we create a model that maps the input image to the activations\n",
        "  # of the last conv layer as well as the output predictions\n",
        "  grad_model = tf.keras.models.Model(\n",
        "      [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "  )\n",
        "\n",
        "  # Then, we compute the gradient of the top predicted class for our input image\n",
        "  # with respect to the activations of the last conv layer\n",
        "  with tf.GradientTape() as tape:\n",
        "      last_conv_layer_output, preds = grad_model(img_array)\n",
        "      if pred_index is None:\n",
        "          pred_index = tf.argmax(preds[0])\n",
        "      class_channel = preds[:, pred_index]\n",
        "\n",
        "  # This is the gradient of the output neuron (top predicted or chosen)\n",
        "  # with regard to the output feature map of the last conv layer\n",
        "  grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "  # This is a vector where each entry is the mean intensity of the gradient\n",
        "  # over a specific feature map channel\n",
        "  pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "  # We multiply each channel in the feature map array\n",
        "  # by \"how important this channel is\" with regard to the top predicted class\n",
        "  # then sum all the channels to obtain the heatmap class activation\n",
        "  last_conv_layer_output = last_conv_layer_output[0]\n",
        "  heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "  heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "  # The absolute value of the gradient shows the effect of change at each pixel\n",
        "  # Source: https://christophm.github.io/interpretable-ml-book/pixel-attribution.html\n",
        "  heatmap = tf.abs(heatmap)\n",
        "\n",
        "  # Normalize to between 0 and 1 (use epsilon, a very small float, to prevent divide-by-zero error)\n",
        "  heatmap_min = np.min(heatmap)\n",
        "  heatmap_max = np.max(heatmap)\n",
        "  heatmap = (heatmap - heatmap_min) / (heatmap_max - heatmap_min + tf.keras.backend.epsilon())\n",
        "\n",
        "  return heatmap.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CLU0fKpoSvv"
      },
      "source": [
        "### We need to tell Grad-CAM where to find the last convolution layer\n",
        "\n",
        "# Print out the layers in the model\n",
        "for layer in model.layers:\n",
        "  print(layer, layer.name)\n",
        "\n",
        "# Go backwards through the model to find the last convolution layer\n",
        "last_conv_layer = None\n",
        "for layer in reversed(model.layers):\n",
        "    if 'conv' in layer.name:\n",
        "        last_conv_layer = layer.name\n",
        "        break\n",
        "\n",
        "# Give a warning if the last convolution layer could not be found\n",
        "if last_conv_layer is not None:\n",
        "  print(\"Last convolution layer found:\", last_conv_layer)\n",
        "else:\n",
        "  print(\"ERROR: Last convolution layer could not be found. Do not continue.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1fxqDUjnK4K"
      },
      "source": [
        "### Generate class activation heatmap\n",
        "heatmap = make_gradcam_heatmap(images, model, last_conv_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaMUlMGYoNYi"
      },
      "source": [
        "### Draw map\n",
        "plt.imshow(heatmap, cmap='jet', vmin=0.0, vmax=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpBNKFivsKij"
      },
      "source": [
        "### Overlay the saliency map on top of the original input image\n",
        "\n",
        "# The heatmap is a lot smaller than the original image, so we upsample it\n",
        "big_heatmap = cv2.resize(heatmap, dsize=(HEIGHT, WIDTH), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# Draw original image with heatmap superimposed over it\n",
        "idx = 0\n",
        "ax = plt.subplot()\n",
        "ax.imshow(images[idx,:,:,0], cmap='gray', vmin=0.0, vmax=1.0)\n",
        "ax.imshow(big_heatmap, cmap='jet', alpha=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}